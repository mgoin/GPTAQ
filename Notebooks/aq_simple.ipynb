{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43871e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n",
      "env: TRANSFORMERS_CACHE=/mnt/LLM/hub\n",
      "env: OMP_NUM_THREADS=16\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2\n",
    "%env TRANSFORMERS_CACHE=/mnt/LLM/hub\n",
    "%env OMP_NUM_THREADS=16\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import time\n",
    "import random\n",
    "from tqdm.auto import trange\n",
    "import ipynbname  # pip install ipynbname\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "\n",
    "from src.aq import QuantizedWeight\n",
    "\n",
    "\n",
    "torch.set_num_threads(16)\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "input_loading_dir = '/extra_disk_1/vahe1994/BRRR/layer10.self_attn.q_proj.input_activation.pt'\n",
    "num_codebooks = 1\n",
    "nbits_per_codebook = 14\n",
    "out_group_size = 1\n",
    "in_group_size = 8\n",
    "batch_size = 16384\n",
    "beam_size = 1\n",
    "beam_search_epochs = 100\n",
    "sparsity_regularizer = 0\n",
    "print_frequency = 10\n",
    "scale_nbits = 2    # 0 means no scales, 16 means no compression;\n",
    "codebook_values_nbits = 16  # less than 16 means we quantize codebooks as well\n",
    "init_max_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7e6156a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjustheuristic\u001b[0m (\u001b[33mrock-and-roll\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jheuristic/GPTAQ_tp/Notebooks/wandb/run-20231213_045551-vmtod41j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rock-and-roll/AddQuantization-debug/runs/vmtod41j' target=\"_blank\">aq_simple_AQ_num_codebooks=1_out_group_size=1_in_group_size=8_nbits_per_codebook=14_beam_search_epochs=100</a></strong> to <a href='https://wandb.ai/rock-and-roll/AddQuantization-debug' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rock-and-roll/AddQuantization-debug' target=\"_blank\">https://wandb.ai/rock-and-roll/AddQuantization-debug</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rock-and-roll/AddQuantization-debug/runs/vmtod41j' target=\"_blank\">https://wandb.ai/rock-and-roll/AddQuantization-debug/runs/vmtod41j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No relevant files were detected in the specified directory. No code will be logged to your run.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = os.path.join(os.getcwd(), ipynbname.name() + \".ipynb\")\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "run = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    dir=os.getcwd(),\n",
    "    project=\"AddQuantization-debug\",\n",
    "    entity = \"rock-and-roll\",\n",
    "    save_code=True,\n",
    "    name = f\"{ipynbname.name()}_AQ_{num_codebooks=}_{out_group_size=}_{in_group_size=}_{nbits_per_codebook=}_{beam_search_epochs=}\",\n",
    "    settings=wandb.Settings(code_dir=\".\"),\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"num_codebooks\" : num_codebooks,\n",
    "    \"out_group_size\": out_group_size,\n",
    "    \"in_group_size\": in_group_size,\n",
    "    \"group_size\" : out_group_size * in_group_size,\n",
    "    \"batch_size\" : batch_size,\n",
    "    \"beam_size\" : beam_size,\n",
    "    \"nbits_per_codebook\" : nbits_per_codebook,\n",
    "    \"codebook_values_nbits\": codebook_values_nbits,\n",
    "    \"scale_nbits\": scale_nbits,\n",
    "    \"beam_search_epochs\": beam_search_epochs,\n",
    "    \"sparsity_regularizer\": sparsity_regularizer,\n",
    "    \"init_max_iter\": init_max_iter,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf7e1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa252766a1ce45979e07c0f7b8184a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-70b-hf\", torch_dtype='auto', low_cpu_mem_usage=True)\n",
    "\n",
    "X = torch.load(input_loading_dir, map_location='cpu').float().flatten(0, -2)\n",
    "reference_weight = model.model.layers[10].self_attn.q_proj.weight.detach().to(device).float()\n",
    "\n",
    "XTX = torch.zeros(X.shape[-1], X.shape[-1], device=device, dtype=torch.float64)\n",
    "for i in range(0, len(X), batch_size):\n",
    "    x_batch = X[i: i + batch_size].cuda().double()\n",
    "    XTX.addmm_(x_batch.T, x_batch, alpha=1/len(X))\n",
    "    del x_batch\n",
    "XTX = XTX.float()\n",
    "del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e8656f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jheuristic/GPTAQ_tp/Notebooks/../src/kmeans.py:223: UserWarning: torch.searchsorted(): boundary tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous boundary tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392022560/work/aten/src/ATen/native/BucketizationUtils.h:38.)\n",
      "  groupwise_cluster_indices = torch.searchsorted(border_indices[:, 1:], groupwise_ranks_1based, side='left')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e40b83309774428b8642b6191f94c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "initializing with kmeans:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jheuristic/GPTAQ_tp/Notebooks/../src/aq.py:559: UserWarning: index_reduce() is in beta and the API may change at any time. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392022560/work/aten/src/ATen/native/cuda/Indexing.cu:1193.)\n",
      "  codebook_i, _, _ = fit_kmeans(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=0.0254404022\t time_on_epoch 0 = 1.802529029082507\n",
      "loss=0.0210026318\t time_on_epoch 10 = 0.13315559295006096\n",
      "loss=0.0175706957\t time_on_epoch 20 = 0.13393667386844754\n",
      "loss=0.0149330052\t time_on_epoch 30 = 0.13344294298440218\n",
      "loss=0.0129503049\t time_on_epoch 40 = 0.13311199308373034\n",
      "loss=0.0114855729\t time_on_epoch 50 = 0.13305627298541367\n",
      "loss=0.0104046396\t time_on_epoch 60 = 0.13350988295860589\n",
      "loss=0.0096008013\t time_on_epoch 70 = 0.1330640739761293\n",
      "loss=0.0089960344\t time_on_epoch 80 = 0.1331505631096661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 0 is less than current step: 1. Dropping entry: {'loss': 0.025440402197482403, '_timestamp': 1702432730.4769356}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=0.0085348955\t time_on_epoch 90 = 0.13324559293687344\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad14f29fc5394c47aee623359ab00c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=0.0035342623\t time_on_epoch 100 = 0.1342155139427632\n",
      "loss=0.0035245917\t time_on_epoch 110 = 0.13356428290717304\n",
      "loss=0.0035197623\t time_on_epoch 120 = 0.13342554285191\n",
      "loss=0.0035165187\t time_on_epoch 130 = 0.13311086385510862\n",
      "loss=0.0035141055\t time_on_epoch 140 = 0.13374247308820486\n",
      "loss=0.0035121871\t time_on_epoch 150 = 0.13366003311239183\n",
      "loss=0.0035105889\t time_on_epoch 160 = 0.13311564410105348\n",
      "loss=0.0035092115\t time_on_epoch 170 = 0.13361279317177832\n",
      "loss=0.0035079945\t time_on_epoch 180 = 0.1335298928897828\n",
      "loss=0.0035068988\t time_on_epoch 190 = 0.13351217308081686\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334edb6be07d43338108526523715190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=0.0027583639\t time_on_epoch 200 = 0.13422761298716068\n",
      "loss=0.0027521192\t time_on_epoch 210 = 0.1331038640346378\n",
      "loss=0.0027489219\t time_on_epoch 220 = 0.1332344738766551\n",
      "loss=0.0027466409\t time_on_epoch 230 = 0.13368744309991598\n",
      "loss=0.0027448470\t time_on_epoch 240 = 0.13358768401667476\n",
      "loss=0.0027433508\t time_on_epoch 250 = 0.13345329486764967\n",
      "loss=0.0027420536\t time_on_epoch 260 = 0.13358681416139007\n",
      "loss=0.0027408986\t time_on_epoch 270 = 0.133119055069983\n",
      "loss=0.0027398508\t time_on_epoch 280 = 0.13355062482878566\n",
      "loss=0.0027388872\t time_on_epoch 290 = 0.13312221597880125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ee644ee56a42ee903654ab6b148242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=0.0025228964\t time_on_epoch 300 = 0.13364888890646398\n",
      "loss=0.0025195496\t time_on_epoch 310 = 0.1336160791106522\n",
      "loss=0.0025177463\t time_on_epoch 320 = 0.13310001883655787\n",
      "loss=0.0025164050\t time_on_epoch 330 = 0.1330654399935156\n",
      "loss=0.0025153146\t time_on_epoch 340 = 0.1332611700054258\n",
      "loss=0.0025143814\t time_on_epoch 350 = 0.13354766997508705\n",
      "loss=0.0025135557\t time_on_epoch 360 = 0.1331644500605762\n",
      "loss=0.0025128087\t time_on_epoch 370 = 0.13312125112861395\n",
      "loss=0.0025121221\t time_on_epoch 380 = 0.1331358510069549\n",
      "loss=0.0025114841\t time_on_epoch 390 = 0.13362607080489397\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c1b5cb6e7044239a8361b7d6c7bfce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=0.0024125674\t time_on_epoch 400 = 0.13408855418674648\n",
      "loss=0.0024103773\t time_on_epoch 410 = 0.13317511486820877\n",
      "loss=0.0024091397\t time_on_epoch 420 = 0.13309856480918825\n",
      "loss=0.0024081878\t time_on_epoch 430 = 0.13308660499751568\n",
      "loss=0.0024073935\t time_on_epoch 440 = 0.13323473604395986\n",
      "loss=0.0024066997\t time_on_epoch 450 = 0.1335714349988848\n",
      "loss=0.0024060761\t time_on_epoch 460 = 0.1331613960210234\n",
      "loss=0.0024055047\t time_on_epoch 470 = 0.13312953617423773\n",
      "loss=0.0024049743\t time_on_epoch 480 = 0.13308153697289526\n",
      "loss=0.0024044772\t time_on_epoch 490 = 0.13308247597888112\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214d9caaa399404c97c532ce3df11698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=0.0023482013\t time_on_epoch 500 = 0.13381659891456366\n",
      "loss=0.0023466351\t time_on_epoch 510 = 0.13306462997570634\n",
      "loss=0.0023457173\t time_on_epoch 520 = 0.13385432003997266\n",
      "loss=0.0023449968\t time_on_epoch 530 = 0.13312284089624882\n",
      "loss=0.0023443863\t time_on_epoch 540 = 0.13309816014952958\n",
      "loss=0.0023438464\t time_on_epoch 550 = 0.1331249310169369\n",
      "loss=0.0023433560\t time_on_epoch 560 = 0.13319928105920553\n",
      "loss=0.0023429029\t time_on_epoch 570 = 0.13310051104053855\n",
      "loss=0.0023424790\t time_on_epoch 580 = 0.13313604099676013\n",
      "loss=0.0023420793\t time_on_epoch 590 = 0.13310815207660198\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e101b71bded445b5ac2544315a0b86ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=0.0023061153\t time_on_epoch 600 = 0.13407287397421896\n",
      "loss=0.0023049505\t time_on_epoch 610 = 0.13359928503632545\n",
      "loss=0.0023042411\t time_on_epoch 620 = 0.1331018649507314\n",
      "loss=0.0023036717\t time_on_epoch 630 = 0.13314920500852168\n",
      "loss=0.0023031810\t time_on_epoch 640 = 0.1330852450337261\n",
      "loss=0.0023027412\t time_on_epoch 650 = 0.13313689501956105\n",
      "loss=0.0023023376\t time_on_epoch 660 = 0.13315646490082145\n",
      "loss=0.0023019614\t time_on_epoch 670 = 0.13312553614377975\n",
      "loss=0.0023016072\t time_on_epoch 680 = 0.1330747460015118\n",
      "loss=0.0023012713\t time_on_epoch 690 = 0.13316271593794227\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3ea6132eb44b5a99e79c833eb913ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=0.0022764155\t time_on_epoch 700 = 0.13415284804068506\n",
      "loss=0.0022754879\t time_on_epoch 710 = 0.13334769988432527\n",
      "loss=0.0022749107\t time_on_epoch 720 = 0.13311076909303665\n",
      "loss=0.0022744418\t time_on_epoch 730 = 0.13310146005824208\n",
      "loss=0.0022740339\t time_on_epoch 740 = 0.13317885994911194\n",
      "loss=0.0022736655\t time_on_epoch 750 = 0.1330872098915279\n",
      "loss=0.0022733253\t time_on_epoch 760 = 0.1330617901403457\n",
      "loss=0.0022730067\t time_on_epoch 770 = 0.13310838001780212\n",
      "loss=0.0022727052\t time_on_epoch 780 = 0.13305363105610013\n",
      "loss=0.0022724182\t time_on_epoch 790 = 0.13366883993148804\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f63a5f65214defbf053f7ae3781032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=0.0022542918\t time_on_epoch 800 = 0.13398651196621358\n",
      "loss=0.0022535495\t time_on_epoch 810 = 0.1330864131450653\n",
      "loss=0.0022530730\t time_on_epoch 820 = 0.13310854299925268\n",
      "loss=0.0022526800\t time_on_epoch 830 = 0.13312069419771433\n",
      "loss=0.0022523345\t time_on_epoch 840 = 0.13311032392084599\n",
      "loss=0.0022520199\t time_on_epoch 850 = 0.1331010339781642\n",
      "loss=0.0022517274\t time_on_epoch 860 = 0.13319650408811867\n",
      "loss=0.0022514521\t time_on_epoch 870 = 0.1330987939145416\n",
      "loss=0.0022511904\t time_on_epoch 880 = 0.13309921487234533\n",
      "loss=0.0022509403\t time_on_epoch 890 = 0.13314738497138023\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c84cfe9434449148b746ccfe9e61cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=0.0022368615\t time_on_epoch 900 = 0.13390998705290258\n",
      "loss=0.0022362412\t time_on_epoch 910 = 0.1330994670279324\n",
      "loss=0.0022358361\t time_on_epoch 920 = 0.13309631682932377\n",
      "loss=0.0022354991\t time_on_epoch 930 = 0.1331188678741455\n",
      "loss=0.0022352007\t time_on_epoch 940 = 0.1332482979632914\n",
      "loss=0.0022349277\t time_on_epoch 950 = 0.1331654479727149\n",
      "loss=0.0022346728\t time_on_epoch 960 = 0.13312231819145381\n",
      "loss=0.0022344319\t time_on_epoch 970 = 0.13314944808371365\n",
      "loss=0.0022342022\t time_on_epoch 980 = 0.13314449903555214\n",
      "loss=0.0022339821\t time_on_epoch 990 = 0.13316197786480188\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf3199ac67245b9babfb580e910f064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantized_weight = QuantizedWeight(\n",
    "    reference_weight=reference_weight, num_codebooks=num_codebooks,\n",
    "    nbits_per_codebook=nbits_per_codebook, scale_nbits=scale_nbits,\n",
    "    out_group_size=out_group_size, in_group_size=in_group_size,\n",
    "    verbose=True, max_iter=init_max_iter,   # faster init, not tested\n",
    ")\n",
    "run.log({\"Avg_bits\": quantized_weight.estimate_nbits_per_parameter()})\n",
    "opt = torch.optim.Adam(quantized_weight.parameters(), lr=1e-4, betas=(0.0, 0.95), amsgrad=True)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    start = time.perf_counter()\n",
    "    delta_weight = (quantized_weight() - reference_weight).double()\n",
    "    loss = (delta_weight @ XTX.double()).flatten() @ delta_weight.flatten() / len(delta_weight)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    run.log({'loss':loss.item()}, step=epoch)\n",
    "    \n",
    "    if epoch % print_frequency == 0:\n",
    "        print(f\"loss={loss.item():.10f}\\t\",\n",
    "              f\"time_on_epoch {epoch} = {time.perf_counter() - start}\")\n",
    "    if (epoch + 1) % beam_search_epochs == 0:\n",
    "        quantized_weight.beam_search_update_codes_(\n",
    "            XTX, reference_weight, beam_size=beam_size, sparsity_regularizer=sparsity_regularizer,\n",
    "            dim_rng=random.Random(), verbose=True)\n",
    "\n",
    "        if sparsity_regularizer != 0:\n",
    "            sparsity_rate = ((quantized_weight.codes == 0).sum() / quantized_weight.codes.numel()).item()\n",
    "            print(f\"Sparsity rate {sparsity_rate:.5f}\")\n",
    "            run.log({'sparsity rate': sparsity_rate}, step=epoch)\n",
    "            mean_code_nbits = sum(get_mean_nbits_by_codebook(quantized_weight.codes)) / num_codebooks\n",
    "            print(f\"mean_code_nbits {mean_code_nbits:.5f}\")\n",
    "            run.log({'Mean codebook length nbits': mean_code_nbits}, step=epoch)\n",
    "            if in_group_size > 1 and out_group_size > 1:\n",
    "                curr_avg_bits  = calc_avg_bits(num_codebooks, 1, mean_code_nbits,\n",
    "                                     nbits_per_codebook, in_features, out_features, scale_nbits)\n",
    "                run.log({\"Avg_bits\": curr_avg_bits}, step=epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
