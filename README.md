# GPTAQ
Additive quantization for LLM

How to run:
TRANSFORMERS_CACHE="" CUDA_VISIBLE_DEVICES= OMP_NUM_THREADS=16 MKL_NUM_THREADS=16 python main.py  params